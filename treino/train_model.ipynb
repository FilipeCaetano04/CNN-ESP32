{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e910a6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b8bf2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 36576 files belonging to 36 classes.\n",
      "Using 29261 files for training.\n",
      "Found 36576 files belonging to 36 classes.\n",
      "Using 7315 files for validation.\n",
      "Classes encontradas: ['Sample001', 'Sample002', 'Sample003', 'Sample004', 'Sample005', 'Sample006', 'Sample007', 'Sample008', 'Sample009', 'Sample010', 'Sample011', 'Sample012', 'Sample013', 'Sample014', 'Sample015', 'Sample016', 'Sample017', 'Sample018', 'Sample019', 'Sample020', 'Sample021', 'Sample022', 'Sample023', 'Sample024', 'Sample025', 'Sample026', 'Sample027', 'Sample028', 'Sample029', 'Sample030', 'Sample031', 'Sample032', 'Sample033', 'Sample034', 'Sample035', 'Sample036']\n",
      "Total de classes: 36\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "The added layer must be an instance of class Layer. Received: layer=<RandomRotation name=random_rotation, built=False> of type <class 'keras.src.layers.preprocessing.image_preprocessing.random_rotation.RandomRotation'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 31\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal de classes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_ds\u001b[38;5;241m.\u001b[39mclass_names)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m#daqui\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m data_augmentation \u001b[38;5;241m=\u001b[39m \u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSequential\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m  \u001b[49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRandomRotation\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Rotação leve (5%) para placas levemente inclinadas\u001b[39;49;00m\n\u001b[0;32m     33\u001b[0m \u001b[43m  \u001b[49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRandomZoom\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# Zoom (10%) para simular distâncias variadas\u001b[39;49;00m\n\u001b[0;32m     34\u001b[0m \u001b[43m  \u001b[49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRandomTranslation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheight_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Pequenos deslocamentos\u001b[39;49;00m\n\u001b[0;32m     35\u001b[0m \u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# 2. Aplique ao seu dataset de treino\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# O 'map' aplica a função em cada lote de imagens\u001b[39;00m\n\u001b[0;32m     39\u001b[0m train_ds \u001b[38;5;241m=\u001b[39m train_ds\u001b[38;5;241m.\u001b[39mmap(\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x, y: (data_augmentation(x, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), y),\n\u001b[0;32m     41\u001b[0m     num_parallel_calls\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mAUTOTUNE\n\u001b[0;32m     42\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\python\\trackable\\base.py:204\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 204\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m previous_value  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\engine\\sequential.py:177\u001b[0m, in \u001b[0;36mSequential.add\u001b[1;34m(self, layer)\u001b[0m\n\u001b[0;32m    175\u001b[0m         layer \u001b[38;5;241m=\u001b[39m functional\u001b[38;5;241m.\u001b[39mModuleWrapper(layer)\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    178\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe added layer must be an instance of class Layer. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    179\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: layer=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(layer)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    180\u001b[0m     )\n\u001b[0;32m    182\u001b[0m tf_utils\u001b[38;5;241m.\u001b[39massert_no_legacy_layers([layer])\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_layer_name_unique(layer):\n",
      "\u001b[1;31mTypeError\u001b[0m: The added layer must be an instance of class Layer. Received: layer=<RandomRotation name=random_rotation, built=False> of type <class 'keras.src.layers.preprocessing.image_preprocessing.random_rotation.RandomRotation'>."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "IMG_SIZE = 64 #tamanho imagem em px\n",
    "CHANNELS = 1 #canais\n",
    "BATCH_SIZE = 32 #tamanho do batch (lote) de treinamento\n",
    "\n",
    "#instância de objeto que extrai o dataset de imagens a partir de sistema de pasta e subpastas (pasta principal = dataset, nome das subpastas = classe, imagens dentro das subpastas = amostras )\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    r'C:\\Users\\paulo\\OneDrive\\Área de Trabalho\\PDI_CNN\\placas',\n",
    "    validation_split=0.2, #20% pra validação - 80% pra treino\n",
    "    subset=\"training\",\n",
    "    seed=123, #seed pra conectar a divisão de dataset de treino e teste\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    color_mode='grayscale'\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    r'C:\\Users\\paulo\\OneDrive\\Área de Trabalho\\PDI_CNN\\placas',\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=123,\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    color_mode='grayscale'\n",
    ")\n",
    "print(f\"Classes encontradas: {train_ds.class_names}\")\n",
    "print(f\"Total de classes: {len(train_ds.class_names)}\")\n",
    "\n",
    "#normalização dos pixels das imagens\n",
    "normalization_layer = layers.Rescaling(1./127.5, offset=-1) #objeto de normalização, pixels entre 1 e -1\n",
    "train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y)) #aplicação no dataset treino. map aplica a função lambda de normalização\n",
    "val_ds = val_ds.map(lambda x, y: (normalization_layer(x), y)) #mesmo para teste/val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5863a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando Treinamento com QAT...\n",
      "Epoch 1/30\n",
      "915/915 [==============================] - 56s 59ms/step - loss: 3.4124 - accuracy: 0.0736 - val_loss: 2.9389 - val_accuracy: 0.2395\n",
      "Epoch 2/30\n",
      "915/915 [==============================] - 18s 20ms/step - loss: 2.7095 - accuracy: 0.2459 - val_loss: 2.2165 - val_accuracy: 0.4608\n",
      "Epoch 3/30\n",
      "915/915 [==============================] - 11s 12ms/step - loss: 2.2279 - accuracy: 0.3805 - val_loss: 1.8153 - val_accuracy: 0.5656\n",
      "Epoch 4/30\n",
      "915/915 [==============================] - 11s 12ms/step - loss: 1.9283 - accuracy: 0.4641 - val_loss: 1.5352 - val_accuracy: 0.6405\n",
      "Epoch 5/30\n",
      "915/915 [==============================] - 11s 12ms/step - loss: 1.6857 - accuracy: 0.5363 - val_loss: 1.3558 - val_accuracy: 0.6775\n",
      "Epoch 6/30\n",
      "915/915 [==============================] - 11s 12ms/step - loss: 1.5294 - accuracy: 0.5836 - val_loss: 1.1948 - val_accuracy: 0.7152\n",
      "Epoch 7/30\n",
      "915/915 [==============================] - 11s 12ms/step - loss: 1.3922 - accuracy: 0.6198 - val_loss: 1.0678 - val_accuracy: 0.7464\n",
      "Epoch 8/30\n",
      "915/915 [==============================] - 11s 12ms/step - loss: 1.2867 - accuracy: 0.6537 - val_loss: 0.9963 - val_accuracy: 0.7609\n",
      "Epoch 9/30\n",
      "915/915 [==============================] - 11s 12ms/step - loss: 1.1765 - accuracy: 0.6818 - val_loss: 0.9373 - val_accuracy: 0.7666\n",
      "Epoch 10/30\n",
      "915/915 [==============================] - 11s 12ms/step - loss: 1.1046 - accuracy: 0.7006 - val_loss: 0.8301 - val_accuracy: 0.7973\n",
      "Epoch 11/30\n",
      "915/915 [==============================] - 11s 12ms/step - loss: 1.0351 - accuracy: 0.7171 - val_loss: 0.7904 - val_accuracy: 0.8083\n",
      "Epoch 12/30\n",
      "915/915 [==============================] - 11s 12ms/step - loss: 0.9717 - accuracy: 0.7350 - val_loss: 0.7184 - val_accuracy: 0.8242\n",
      "Epoch 13/30\n",
      "915/915 [==============================] - 12s 13ms/step - loss: 0.9298 - accuracy: 0.7467 - val_loss: 0.7028 - val_accuracy: 0.8271\n",
      "Epoch 14/30\n",
      "915/915 [==============================] - 13s 14ms/step - loss: 0.9056 - accuracy: 0.7512 - val_loss: 0.6706 - val_accuracy: 0.8313\n",
      "Epoch 15/30\n",
      "915/915 [==============================] - 12s 13ms/step - loss: 0.8941 - accuracy: 0.7510 - val_loss: 0.6457 - val_accuracy: 0.8335\n",
      "Epoch 16/30\n",
      "915/915 [==============================] - 11s 12ms/step - loss: 0.8471 - accuracy: 0.7645 - val_loss: 0.6496 - val_accuracy: 0.8306\n",
      "Epoch 17/30\n",
      "915/915 [==============================] - 12s 13ms/step - loss: 0.8232 - accuracy: 0.7692 - val_loss: 0.6036 - val_accuracy: 0.8461\n",
      "Epoch 18/30\n",
      "915/915 [==============================] - 11s 12ms/step - loss: 0.7988 - accuracy: 0.7804 - val_loss: 0.5970 - val_accuracy: 0.8455\n",
      "Epoch 19/30\n",
      "915/915 [==============================] - 11s 12ms/step - loss: 0.7787 - accuracy: 0.7839 - val_loss: 0.5658 - val_accuracy: 0.8519\n",
      "Epoch 20/30\n",
      "915/915 [==============================] - 12s 13ms/step - loss: 0.7607 - accuracy: 0.7887 - val_loss: 0.5557 - val_accuracy: 0.8567\n",
      "Epoch 21/30\n",
      "915/915 [==============================] - 12s 13ms/step - loss: 0.7430 - accuracy: 0.7902 - val_loss: 0.5642 - val_accuracy: 0.8530\n",
      "Epoch 22/30\n",
      "915/915 [==============================] - 13s 14ms/step - loss: 0.7296 - accuracy: 0.7935 - val_loss: 0.5314 - val_accuracy: 0.8604\n",
      "Epoch 23/30\n",
      "915/915 [==============================] - 12s 13ms/step - loss: 0.7275 - accuracy: 0.7947 - val_loss: 0.5193 - val_accuracy: 0.8634\n",
      "Epoch 24/30\n",
      "915/915 [==============================] - 12s 13ms/step - loss: 0.7039 - accuracy: 0.8013 - val_loss: 0.5202 - val_accuracy: 0.8653\n",
      "Epoch 25/30\n",
      "915/915 [==============================] - 12s 13ms/step - loss: 0.6962 - accuracy: 0.8037 - val_loss: 0.5237 - val_accuracy: 0.8636\n",
      "Epoch 26/30\n",
      "915/915 [==============================] - 15s 17ms/step - loss: 0.6734 - accuracy: 0.8106 - val_loss: 0.4883 - val_accuracy: 0.8726\n",
      "Epoch 27/30\n",
      "915/915 [==============================] - 12s 13ms/step - loss: 0.6733 - accuracy: 0.8092 - val_loss: 0.4863 - val_accuracy: 0.8700\n",
      "Epoch 28/30\n",
      "915/915 [==============================] - 15s 17ms/step - loss: 0.6579 - accuracy: 0.8157 - val_loss: 0.4701 - val_accuracy: 0.8753\n",
      "Epoch 29/30\n",
      "915/915 [==============================] - 13s 15ms/step - loss: 0.6412 - accuracy: 0.8156 - val_loss: 0.4724 - val_accuracy: 0.8729\n",
      "Epoch 30/30\n",
      "915/915 [==============================] - 12s 13ms/step - loss: 0.6423 - accuracy: 0.8211 - val_loss: 0.4473 - val_accuracy: 0.8804\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x17d9ada5940>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tf_keras as keras\n",
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "#criação da cnn por camadas\n",
    "def create_esp32_simple_cnn():\n",
    "    inputs = keras.Input(shape=(64, 64, 1)) #camada de entrada com imagens 64x64x1\n",
    "    \n",
    "    x = keras.layers.Conv2D(16, (3, 3), strides=(2, 2), padding='same', activation='relu')(inputs) #camada convolucional (16 filtros de tamanho 3x3, com passos 2x2, padding de 0 para ir às bordas). 32x32x16 \n",
    "    \n",
    "    x = keras.layers.SeparableConv2D(32, (3, 3), strides=(2, 2), padding='same', activation='relu')(x) #convolução separável com 32 filtros (conceito de mobilenet), aqui as convoluções são feitas em plano e profundidade separadamente. 16x16x32\n",
    "    \n",
    "    x = keras.layers.SeparableConv2D(64, (3, 3), strides=(2, 2), padding='same', activation='relu')(x) #convolução separável com 64 filtros. 8x8x64\n",
    "    \n",
    "    x = keras.layers.GlobalAveragePooling2D()(x) #camada de pooling, aqui cada um dos 64 mapas de características tem sua média global retirada. gerando 1 número por mapa\n",
    "    x = keras.layers.Dropout(0.2)(x) #desligamento de 20% dos neurônios aleatoriamente durante o treino para a rede não depender de neurônios específicos, evita overfiting\n",
    "    \n",
    "    outputs = keras.layers.Dense(36, activation='softmax')(x) #camada de saida com 36 neurônios (0-9, A-Z), ativação softmax retorna probabilidades\n",
    "    \n",
    "    return keras.Model(inputs=inputs, outputs=outputs) #retorno do modelo\n",
    "\n",
    "base_model = create_esp32_simple_cnn() #criando modelo\n",
    "\n",
    "q_aware_model = tfmot.quantization.keras.quantize_model(base_model) #faz o modelo reconhecer que será quantizado - simula 'erros' de arredondamento para que o backpropagation ajuste os pesos de modo a compensar a posterior quantização\n",
    "\n",
    "q_aware_model.compile(\n",
    "    optimizer='adam', #ajusta learning rate para cada neurônio individualmente baseado na necessidade\n",
    "    loss='sparse_categorical_crossentropy', #definição da função de custo\n",
    "    metrics=['accuracy'] #porcentagem de imagens que o modelo está acertando\n",
    ")\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping( #parada antecipada\n",
    "    monitor='val_loss', #valor monitorado \n",
    "    patience=5, # se por 5 épocas a perda não cair, ele para\n",
    "    restore_best_weights=True # garante que o modelo final seja o melhor de todos\n",
    ")\n",
    "\n",
    "print(\"Iniciando Treinamento com QAT...\")\n",
    "q_aware_model.fit(train_ds, validation_data=val_ds, epochs=30, callbacks = [early_stop]) #treinamento do modelo com QAT, até 30 épocas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f0e43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\paulo\\AppData\\Local\\Temp\\tmpbj3sc9yp\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\paulo\\AppData\\Local\\Temp\\tmpbj3sc9yp\\assets\n",
      "C:\\Users\\paulo\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\lite\\python\\convert.py:863: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo convertido com sucesso para modelo_placa_int8.tflite!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#um gerador de dados representativos, pega amostras para determinar os valores máximos e mínimos das ativações de cada camada, a fim de converter para 8 bits sem perder informação\n",
    "def representative_data_gen():\n",
    "    for input_value, _ in val_ds.take(100): #carrega 100 imagens reais do dataset, for input_value, _ (é uma tupla de iteração input_value = x, _ = y)\n",
    "        yield [input_value] #retorno com yield é um iterador, itera sobre cada input_value/x/imagem\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model) #TFLiteConverter tira dados que são necessários apenas para o treinamento e mantém apenas o necessário para inferência\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT] #Aqui é feito a quantização dos pesos, transforma kernels decimais em int8. converter.optimizations atributo lista\n",
    "converter.representative_dataset = representative_data_gen #Quantização de valores de ativação. converter.representative_dataset atributo que espera função geradora/iteradora. aponta para a referência da função\n",
    "\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8] #Força todas as operações para serem feitas em int8. converter.target_spec.supported_ops subobjeto que guarda regras de operações matemáticas           \n",
    "converter.inference_input_type = tf.int8 #informa que a entrada será de pixels já convertidos para int8\n",
    "converter.inference_output_type = tf.int8 #informa que a saída será de probabilidades já convertidas para int8\n",
    "\n",
    "tflite_model_quant = converter.convert() #Finalizando conversão - String de bytes\n",
    "\n",
    "with open(\"modelo_placa_int8.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model_quant) #escrita do arquivo .tflite no disco \n",
    "\n",
    "print(\"Modelo convertido com sucesso para modelo_placa_int8.tflite!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf2b23a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo modelo_placa_int8.h gerado com sucesso!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def bin_to_header(filename, var_name='modelo_placa_int8'): \n",
    "    with open(filename, 'rb') as f: #abre arquivo no modo read bin\n",
    "        data = f.read() #lê e guarda arquivo .tflite na var data\n",
    "    \n",
    "    with open(var_name + '.h', 'w') as f: #cria arquivo e adiciona extensão .h, modo write\n",
    "        f.write(f'unsigned char {var_name}[] = {{\\n  ') # Escreve a declaração do array em C++: tipo 'unsigned char' (1 byte) para garantir compatibilidade\n",
    "        for i, byte in enumerate(data): #retorna iterador e byte\n",
    "            f.write(f'0x{byte:02x}, ') #escreve o byte em hexadecimal seguido de vírgula, :02x força dois dígitos\n",
    "            if (i + 1) % 12 == 0: #quebra linha a cada 12 bytes\n",
    "                f.write('\\n  ')\n",
    "        f.write('\\n};\\n\\n') #fecha array e quebra linha\n",
    "        f.write(f'unsigned int {var_name}_len = {len(data)};\\n') #guarda uma variável com tamanho em bytes do modelo\n",
    " \n",
    "bin_to_header('modelo_placa_int8.tflite')\n",
    "print(\"Arquivo modelo_placa_int8.h gerado com sucesso!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
