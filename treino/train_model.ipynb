{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e910a6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2b8bf2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 36576 files belonging to 36 classes.\n",
      "Using 29261 files for training.\n",
      "Found 36576 files belonging to 36 classes.\n",
      "Using 7315 files for validation.\n",
      "Classes encontradas: ['Sample001', 'Sample002', 'Sample003', 'Sample004', 'Sample005', 'Sample006', 'Sample007', 'Sample008', 'Sample009', 'Sample010', 'Sample011', 'Sample012', 'Sample013', 'Sample014', 'Sample015', 'Sample016', 'Sample017', 'Sample018', 'Sample019', 'Sample020', 'Sample021', 'Sample022', 'Sample023', 'Sample024', 'Sample025', 'Sample026', 'Sample027', 'Sample028', 'Sample029', 'Sample030', 'Sample031', 'Sample032', 'Sample033', 'Sample034', 'Sample035', 'Sample036']\n",
      "Total de classes: 36\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "IMG_SIZE = 64 #tamanho imagem em px\n",
    "CHANNELS = 1 #canais\n",
    "BATCH_SIZE = 32 #tamanho do batch (lote) de treinamento\n",
    "\n",
    "#instância de objeto que extrai o dataset de imagens a partir de sistema de pasta e subpastas (pasta principal = dataset, nome das subpastas = classe, imagens dentro das subpastas = amostras )\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    r'C:\\Users\\paulo\\OneDrive\\Área de Trabalho\\PDI_CNN\\placas',\n",
    "    validation_split=0.2, #20% pra validação - 80% pra treino\n",
    "    subset=\"training\",\n",
    "    seed=123, #seed pra conectar a divisão de dataset de treino e teste\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    color_mode='grayscale'\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    r'C:\\Users\\paulo\\OneDrive\\Área de Trabalho\\PDI_CNN\\placas',\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=123,\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    color_mode='grayscale'\n",
    ")\n",
    "print(f\"Classes encontradas: {train_ds.class_names}\")\n",
    "print(f\"Total de classes: {len(train_ds.class_names)}\")\n",
    "\n",
    "#normalização dos pixels das imagens\n",
    "normalization_layer = layers.Rescaling(1./127.5, offset=-1) #objeto de normalização, pixels entre 1 e -1\n",
    "train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y)) #aplicação no dataset treino. map aplica a função lambda de normalização\n",
    "val_ds = val_ds.map(lambda x, y: (normalization_layer(x), y)) #mesmo para teste/val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d5863a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando Treinamento com QAT...\n",
      "Epoch 1/30\n",
      "915/915 [==============================] - 50s 53ms/step - loss: 3.4401 - accuracy: 0.0600 - val_loss: 2.9261 - val_accuracy: 0.2211\n",
      "Epoch 2/30\n",
      "915/915 [==============================] - 11s 12ms/step - loss: 2.7577 - accuracy: 0.2231 - val_loss: 2.3604 - val_accuracy: 0.4230\n",
      "Epoch 3/30\n",
      "915/915 [==============================] - 13s 14ms/step - loss: 2.3464 - accuracy: 0.3559 - val_loss: 1.9723 - val_accuracy: 0.5340\n",
      "Epoch 4/30\n",
      "915/915 [==============================] - 12s 13ms/step - loss: 2.0377 - accuracy: 0.4429 - val_loss: 1.6694 - val_accuracy: 0.6144\n",
      "Epoch 5/30\n",
      "915/915 [==============================] - 12s 13ms/step - loss: 1.7788 - accuracy: 0.5205 - val_loss: 1.4176 - val_accuracy: 0.6749\n",
      "Epoch 6/30\n",
      "915/915 [==============================] - 11s 12ms/step - loss: 1.5572 - accuracy: 0.5815 - val_loss: 1.2274 - val_accuracy: 0.7154\n",
      "Epoch 7/30\n",
      "915/915 [==============================] - 13s 14ms/step - loss: 1.3794 - accuracy: 0.6283 - val_loss: 1.1058 - val_accuracy: 0.7357\n",
      "Epoch 8/30\n",
      "915/915 [==============================] - 11s 12ms/step - loss: 1.2520 - accuracy: 0.6633 - val_loss: 0.9798 - val_accuracy: 0.7679\n",
      "Epoch 9/30\n",
      "915/915 [==============================] - 11s 12ms/step - loss: 1.1458 - accuracy: 0.6923 - val_loss: 0.9122 - val_accuracy: 0.7757\n",
      "Epoch 10/30\n",
      "915/915 [==============================] - 11s 12ms/step - loss: 1.0804 - accuracy: 0.7062 - val_loss: 0.8705 - val_accuracy: 0.7904\n",
      "Epoch 11/30\n",
      "915/915 [==============================] - 13s 14ms/step - loss: 1.0163 - accuracy: 0.7260 - val_loss: 0.7928 - val_accuracy: 0.8042\n",
      "Epoch 12/30\n",
      "915/915 [==============================] - 11s 13ms/step - loss: 0.9709 - accuracy: 0.7389 - val_loss: 0.7403 - val_accuracy: 0.8156\n",
      "Epoch 13/30\n",
      "915/915 [==============================] - 11s 12ms/step - loss: 0.9241 - accuracy: 0.7500 - val_loss: 0.7319 - val_accuracy: 0.8217\n",
      "Epoch 14/30\n",
      "915/915 [==============================] - 11s 12ms/step - loss: 0.8846 - accuracy: 0.7611 - val_loss: 0.6799 - val_accuracy: 0.8319\n",
      "Epoch 15/30\n",
      "915/915 [==============================] - 11s 12ms/step - loss: 0.8751 - accuracy: 0.7613 - val_loss: 0.6558 - val_accuracy: 0.8331\n",
      "Epoch 16/30\n",
      "915/915 [==============================] - 11s 12ms/step - loss: 0.8533 - accuracy: 0.7639 - val_loss: 0.6328 - val_accuracy: 0.8387\n",
      "Epoch 17/30\n",
      "915/915 [==============================] - 11s 12ms/step - loss: 0.8291 - accuracy: 0.7694 - val_loss: 0.6198 - val_accuracy: 0.8420\n",
      "Epoch 18/30\n",
      "915/915 [==============================] - 11s 12ms/step - loss: 0.8021 - accuracy: 0.7766 - val_loss: 0.6036 - val_accuracy: 0.8459\n",
      "Epoch 19/30\n",
      "915/915 [==============================] - 11s 12ms/step - loss: 0.7764 - accuracy: 0.7819 - val_loss: 0.5814 - val_accuracy: 0.8528\n",
      "Epoch 20/30\n",
      "915/915 [==============================] - 11s 12ms/step - loss: 0.7584 - accuracy: 0.7891 - val_loss: 0.5713 - val_accuracy: 0.8569\n",
      "Epoch 21/30\n",
      "915/915 [==============================] - 11s 12ms/step - loss: 0.7425 - accuracy: 0.7907 - val_loss: 0.5810 - val_accuracy: 0.8535\n",
      "Epoch 22/30\n",
      "915/915 [==============================] - 11s 12ms/step - loss: 0.7282 - accuracy: 0.7984 - val_loss: 0.5492 - val_accuracy: 0.8615\n",
      "Epoch 23/30\n",
      "915/915 [==============================] - 11s 12ms/step - loss: 0.7112 - accuracy: 0.8007 - val_loss: 0.5303 - val_accuracy: 0.8666\n",
      "Epoch 24/30\n",
      "915/915 [==============================] - 11s 12ms/step - loss: 0.7034 - accuracy: 0.8053 - val_loss: 0.5147 - val_accuracy: 0.8677\n",
      "Epoch 25/30\n",
      "915/915 [==============================] - 11s 13ms/step - loss: 0.6811 - accuracy: 0.8115 - val_loss: 0.4979 - val_accuracy: 0.8719\n",
      "Epoch 26/30\n",
      "915/915 [==============================] - 11s 12ms/step - loss: 0.6720 - accuracy: 0.8129 - val_loss: 0.5176 - val_accuracy: 0.8671\n",
      "Epoch 27/30\n",
      "915/915 [==============================] - 11s 12ms/step - loss: 0.6605 - accuracy: 0.8163 - val_loss: 0.4786 - val_accuracy: 0.8759\n",
      "Epoch 28/30\n",
      "915/915 [==============================] - 11s 12ms/step - loss: 0.6510 - accuracy: 0.8200 - val_loss: 0.4772 - val_accuracy: 0.8749\n",
      "Epoch 29/30\n",
      "915/915 [==============================] - 11s 13ms/step - loss: 0.6424 - accuracy: 0.8188 - val_loss: 0.4696 - val_accuracy: 0.8812\n",
      "Epoch 30/30\n",
      "915/915 [==============================] - 11s 12ms/step - loss: 0.6297 - accuracy: 0.8241 - val_loss: 0.4693 - val_accuracy: 0.8774\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x17d8a6e5b20>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tf_keras as keras\n",
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "#criação da cnn por camadas\n",
    "def create_esp32_simple_cnn():\n",
    "    inputs = keras.Input(shape=(64, 64, 1)) #camada de entrada com imagens 64x64x1\n",
    "    \n",
    "    x = keras.layers.Conv2D(16, (3, 3), strides=(2, 2), padding='same', activation='relu')(inputs) #camada convolucional (16 filtros de tamanho 3x3, com passos 2x2, padding de 0 para ir às bordas). 32x32x16 \n",
    "    \n",
    "    x = keras.layers.SeparableConv2D(32, (3, 3), strides=(2, 2), padding='same', activation='relu')(x) #convolução separável com 32 filtros (conceito de mobilenet), aqui as convoluções são feitas em plano e profundidade separadamente. 16x16x32\n",
    "    \n",
    "    x = keras.layers.SeparableConv2D(64, (3, 3), strides=(2, 2), padding='same', activation='relu')(x) #convolução separável com 64 filtros. 8x8x64\n",
    "    \n",
    "    x = keras.layers.GlobalAveragePooling2D()(x) #camada de pooling, aqui cada um dos 64 mapas de características tem sua média global retirada. gerando 1 número por mapa\n",
    "    x = keras.layers.Dropout(0.2)(x) #desligamento de 20% dos neurônios aleatoriamente durante o treino para a rede não depender de neurônios específicos, evita overfiting\n",
    "    \n",
    "    outputs = keras.layers.Dense(36, activation='softmax')(x) #camada de saida com 36 neurônios (0-9, A-Z), ativação softmax retorna probabilidades\n",
    "    \n",
    "    return keras.Model(inputs=inputs, outputs=outputs) #retorno do modelo\n",
    "\n",
    "base_model = create_esp32_simple_cnn() #criando modelo\n",
    "\n",
    "q_aware_model = tfmot.quantization.keras.quantize_model(base_model) #faz o modelo reconhecer que será quantizado - simula 'erros' de arredondamento para que o backpropagation ajuste os pesos de modo a compensar a posterior quantização\n",
    "\n",
    "q_aware_model.compile(\n",
    "    optimizer='adam', #ajusta learning rate para cada neurônio individualmente baseado na necessidade\n",
    "    loss='sparse_categorical_crossentropy', #definição da função de custo\n",
    "    metrics=['accuracy'] #porcentagem de imagens que o modelo está acertando\n",
    ")\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping( #parada antecipada\n",
    "    monitor='val_loss', #valor monitorado \n",
    "    patience=5, # se por 5 épocas a perda não cair, ele para\n",
    "    restore_best_weights=True # garante que o modelo final seja o melhor de todos\n",
    ")\n",
    "\n",
    "print(\"Iniciando Treinamento com QAT...\")\n",
    "q_aware_model.fit(train_ds, validation_data=val_ds, epochs=30, callbacks = [early_stop]) #treinamento do modelo com QAT, até 30 épocas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76f0e43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\paulo\\AppData\\Local\\Temp\\tmpzbm140qj\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\paulo\\AppData\\Local\\Temp\\tmpzbm140qj\\assets\n",
      "C:\\Users\\paulo\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\lite\\python\\convert.py:863: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo convertido com sucesso para modelo_placa_int8.tflite!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#um gerador de dados representativos, pega amostras para determinar os valores máximos e mínimos das ativações de cada camada, a fim de converter para 8 bits sem perder informação\n",
    "def representative_data_gen():\n",
    "    for input_value, _ in val_ds.take(100): #carrega 100 imagens reais do dataset, for input_value, _ (é uma tupla de iteração input_value = x, _ = y)\n",
    "        yield [input_value] #retorno com yield é um iterador, itera sobre cada input_value/x/imagem\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model) #TFLiteConverter tira dados que são necessários apenas para o treinamento e mantém apenas o necessário para inferência\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT] #Aqui é feito a quantização dos pesos, transforma kernels decimais em int8. converter.optimizations atributo lista\n",
    "converter.representative_dataset = representative_data_gen #Quantização de valores de ativação. converter.representative_dataset atributo que espera função geradora/iteradora. aponta para a referência da função\n",
    "\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8] #Força todas as operações para serem feitas em int8. converter.target_spec.supported_ops subobjeto que guarda regras de operações matemáticas           \n",
    "converter.inference_input_type = tf.int8 #informa que a entrada será de pixels já convertidos para int8\n",
    "converter.inference_output_type = tf.int8 #informa que a saída será de probabilidades já convertidas para int8\n",
    "\n",
    "tflite_model_quant = converter.convert() #Finalizando conversão - String de bytes\n",
    "\n",
    "with open(\"modelo_placa_int8.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model_quant) #escrita do arquivo .tflite no disco \n",
    "\n",
    "print(\"Modelo convertido com sucesso para modelo_placa_int8.tflite!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebf2b23a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo modelo_placa_int8.h gerado com sucesso!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def bin_to_header(filename, var_name='modelo_placa_int8'): \n",
    "    with open(filename, 'rb') as f: #abre arquivo no modo read bin\n",
    "        data = f.read() #lê e guarda arquivo .tflite na var data\n",
    "    \n",
    "    with open(var_name + '.h', 'w') as f: #cria arquivo e adiciona extensão .h, modo write\n",
    "        f.write(f'unsigned char {var_name}[] = {{\\n  ') # Escreve a declaração do array em C++: tipo 'unsigned char' (1 byte) para garantir compatibilidade\n",
    "        for i, byte in enumerate(data): #retorna iterador e byte\n",
    "            f.write(f'0x{byte:02x}, ') #escreve o byte em hexadecimal seguido de vírgula, :02x força dois dígitos\n",
    "            if (i + 1) % 12 == 0: #quebra linha a cada 12 bytes\n",
    "                f.write('\\n  ')\n",
    "        f.write('\\n};\\n\\n') #fecha array e quebra linha\n",
    "        f.write(f'unsigned int {var_name}_len = {len(data)};\\n') #guarda uma variável com tamanho em bytes do modelo\n",
    " \n",
    "bin_to_header('modelo_placa_int8.tflite')\n",
    "print(\"Arquivo modelo_placa_int8.h gerado com sucesso!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
