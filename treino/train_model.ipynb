{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e910a6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2b8bf2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 36576 files belonging to 36 classes.\n",
      "Using 29261 files for training.\n",
      "Found 36576 files belonging to 36 classes.\n",
      "Using 7315 files for validation.\n",
      "Classes encontradas: ['Sample001', 'Sample002', 'Sample003', 'Sample004', 'Sample005', 'Sample006', 'Sample007', 'Sample008', 'Sample009', 'Sample010', 'Sample011', 'Sample012', 'Sample013', 'Sample014', 'Sample015', 'Sample016', 'Sample017', 'Sample018', 'Sample019', 'Sample020', 'Sample021', 'Sample022', 'Sample023', 'Sample024', 'Sample025', 'Sample026', 'Sample027', 'Sample028', 'Sample029', 'Sample030', 'Sample031', 'Sample032', 'Sample033', 'Sample034', 'Sample035', 'Sample036']\n",
      "Total de classes: 36\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "IMG_SIZE = 64 #tamanho imagem em px\n",
    "CHANNELS = 1 #canais\n",
    "BATCH_SIZE = 32 #tamanho do batch (lote) de treinamento\n",
    "\n",
    "#instância de objeto que extrai o dataset de imagens a partir de sistema de pasta e subpastas (pasta principal = dataset, nome das subpastas = classe, imagens dentro das subpastas = amostras )\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    r'C:\\Users\\paulo\\OneDrive\\Área de Trabalho\\PDI_CNN\\placas',\n",
    "    validation_split=0.2, #20% pra validação - 80% pra treino\n",
    "    subset=\"training\",\n",
    "    seed=123, #seed pra conectar a divisão de dataset de treino e teste\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    color_mode='grayscale'\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    r'C:\\Users\\paulo\\OneDrive\\Área de Trabalho\\PDI_CNN\\placas',\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=123,\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    color_mode='grayscale'\n",
    ")\n",
    "print(f\"Classes encontradas: {train_ds.class_names}\")\n",
    "print(f\"Total de classes: {len(train_ds.class_names)}\")\n",
    "\n",
    "#normalização dos pixels das imagens\n",
    "normalization_layer = layers.Rescaling(1./127.5, offset=-1) #objeto de normalização, pixels entre 1 e -1\n",
    "train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y)) #aplicação no dataset treino. map aplica a função lambda de normalização\n",
    "val_ds = val_ds.map(lambda x, y: (normalization_layer(x), y)) #mesmo para teste/val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d5863a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando Treinamento com QAT...\n",
      "Epoch 1/30\n",
      "915/915 [==============================] - 18s 18ms/step - loss: 3.4762 - accuracy: 0.0558 - val_loss: 3.1828 - val_accuracy: 0.1545\n",
      "Epoch 2/30\n",
      "915/915 [==============================] - 15s 17ms/step - loss: 2.9372 - accuracy: 0.1830 - val_loss: 2.5212 - val_accuracy: 0.3716\n",
      "Epoch 3/30\n",
      "915/915 [==============================] - 15s 16ms/step - loss: 2.4709 - accuracy: 0.3172 - val_loss: 2.0336 - val_accuracy: 0.5183\n",
      "Epoch 4/30\n",
      "915/915 [==============================] - 16s 18ms/step - loss: 2.0348 - accuracy: 0.4413 - val_loss: 1.5847 - val_accuracy: 0.6560\n",
      "Epoch 5/30\n",
      "915/915 [==============================] - 15s 16ms/step - loss: 1.6764 - accuracy: 0.5452 - val_loss: 1.2496 - val_accuracy: 0.7228\n",
      "Epoch 6/30\n",
      "915/915 [==============================] - 15s 16ms/step - loss: 1.4269 - accuracy: 0.6150 - val_loss: 1.0712 - val_accuracy: 0.7530\n",
      "Epoch 7/30\n",
      "915/915 [==============================] - 14s 16ms/step - loss: 1.2640 - accuracy: 0.6571 - val_loss: 0.9679 - val_accuracy: 0.7727\n",
      "Epoch 8/30\n",
      "915/915 [==============================] - 14s 15ms/step - loss: 1.1607 - accuracy: 0.6853 - val_loss: 0.9055 - val_accuracy: 0.7852\n",
      "Epoch 9/30\n",
      "915/915 [==============================] - 14s 15ms/step - loss: 1.0734 - accuracy: 0.7108 - val_loss: 0.8061 - val_accuracy: 0.8037\n",
      "Epoch 10/30\n",
      "915/915 [==============================] - 14s 15ms/step - loss: 1.0147 - accuracy: 0.7251 - val_loss: 0.7556 - val_accuracy: 0.8195\n",
      "Epoch 11/30\n",
      "915/915 [==============================] - 14s 15ms/step - loss: 0.9632 - accuracy: 0.7398 - val_loss: 0.7136 - val_accuracy: 0.8264\n",
      "Epoch 12/30\n",
      "915/915 [==============================] - 14s 15ms/step - loss: 0.9214 - accuracy: 0.7530 - val_loss: 0.6811 - val_accuracy: 0.8313\n",
      "Epoch 13/30\n",
      "915/915 [==============================] - 15s 16ms/step - loss: 0.8749 - accuracy: 0.7637 - val_loss: 0.6580 - val_accuracy: 0.8391\n",
      "Epoch 14/30\n",
      "915/915 [==============================] - 15s 16ms/step - loss: 0.8494 - accuracy: 0.7694 - val_loss: 0.6235 - val_accuracy: 0.8444\n",
      "Epoch 15/30\n",
      "915/915 [==============================] - 15s 16ms/step - loss: 0.8308 - accuracy: 0.7766 - val_loss: 0.6313 - val_accuracy: 0.8483\n",
      "Epoch 16/30\n",
      "915/915 [==============================] - 15s 16ms/step - loss: 0.7895 - accuracy: 0.7862 - val_loss: 0.5794 - val_accuracy: 0.8560\n",
      "Epoch 17/30\n",
      "915/915 [==============================] - 15s 16ms/step - loss: 0.7785 - accuracy: 0.7879 - val_loss: 0.5664 - val_accuracy: 0.8585\n",
      "Epoch 18/30\n",
      "915/915 [==============================] - 16s 17ms/step - loss: 0.7452 - accuracy: 0.7949 - val_loss: 0.5577 - val_accuracy: 0.8574\n",
      "Epoch 19/30\n",
      "915/915 [==============================] - 15s 17ms/step - loss: 0.7333 - accuracy: 0.7979 - val_loss: 0.5325 - val_accuracy: 0.8607\n",
      "Epoch 20/30\n",
      "915/915 [==============================] - 15s 16ms/step - loss: 0.7092 - accuracy: 0.8049 - val_loss: 0.5245 - val_accuracy: 0.8640\n",
      "Epoch 21/30\n",
      "915/915 [==============================] - 33s 37ms/step - loss: 0.6925 - accuracy: 0.8089 - val_loss: 0.5363 - val_accuracy: 0.8611\n",
      "Epoch 22/30\n",
      "915/915 [==============================] - 15s 17ms/step - loss: 0.6882 - accuracy: 0.8117 - val_loss: 0.5084 - val_accuracy: 0.8693\n",
      "Epoch 23/30\n",
      "915/915 [==============================] - 16s 17ms/step - loss: 0.6657 - accuracy: 0.8142 - val_loss: 0.4922 - val_accuracy: 0.8731\n",
      "Epoch 24/30\n",
      "915/915 [==============================] - 16s 17ms/step - loss: 0.6549 - accuracy: 0.8183 - val_loss: 0.4756 - val_accuracy: 0.8764\n",
      "Epoch 25/30\n",
      "915/915 [==============================] - 15s 17ms/step - loss: 0.6466 - accuracy: 0.8179 - val_loss: 0.4607 - val_accuracy: 0.8796\n",
      "Epoch 26/30\n",
      "915/915 [==============================] - 15s 16ms/step - loss: 0.6375 - accuracy: 0.8208 - val_loss: 0.4740 - val_accuracy: 0.8793\n",
      "Epoch 27/30\n",
      "915/915 [==============================] - 15s 16ms/step - loss: 0.6279 - accuracy: 0.8237 - val_loss: 0.4644 - val_accuracy: 0.8744\n",
      "Epoch 28/30\n",
      "915/915 [==============================] - 15s 16ms/step - loss: 0.6148 - accuracy: 0.8270 - val_loss: 0.4483 - val_accuracy: 0.8812\n",
      "Epoch 29/30\n",
      "915/915 [==============================] - 15s 16ms/step - loss: 0.6083 - accuracy: 0.8293 - val_loss: 0.4392 - val_accuracy: 0.8850\n",
      "Epoch 30/30\n",
      "915/915 [==============================] - 15s 16ms/step - loss: 0.6079 - accuracy: 0.8304 - val_loss: 0.4328 - val_accuracy: 0.8852\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x17d8a840e00>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tf_keras as keras\n",
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "#criação da cnn por camadas\n",
    "def create_esp32_simple_cnn():\n",
    "    inputs = keras.Input(shape=(64, 64, 1)) #camada de entrada com imagens 64x64x1\n",
    "    \n",
    "    x = keras.layers.Conv2D(32, (3, 3), strides=(2, 2), padding='same', activation='relu')(inputs) #camada convolucional (32 filtros de tamanho 3x3, com passos 2x2, padding de 0 para ir às bordas). 32x32x32 \n",
    "    \n",
    "    x = keras.layers.SeparableConv2D(32, (3, 3), strides=(2, 2), padding='same', activation='relu')(x) #convolução separável com 32 filtros (conceito de mobilenet), aqui as convoluções são feitas em plano e profundidade separadamente. 16x16x32\n",
    "    \n",
    "    x = keras.layers.SeparableConv2D(64, (3, 3), strides=(2, 2), padding='same', activation='relu')(x) #convolução separável com 64 filtros. 8x8x64\n",
    "    \n",
    "    x = keras.layers.GlobalAveragePooling2D()(x) #camada de pooling, aqui cada um dos 64 mapas de características tem sua média global retirada. gerando 1 número por mapa\n",
    "    x = keras.layers.Dropout(0.2)(x) #desligamento de 20% dos neurônios aleatoriamente durante o treino para a rede não depender de neurônios específicos, evita overfiting\n",
    "    \n",
    "    outputs = keras.layers.Dense(36, activation='softmax')(x) #camada de saida com 36 neurônios (0-9, A-Z), ativação softmax retorna probabilidades\n",
    "    \n",
    "    return keras.Model(inputs=inputs, outputs=outputs) #retorno do modelo\n",
    "\n",
    "base_model = create_esp32_simple_cnn() #criando modelo\n",
    "\n",
    "q_aware_model = tfmot.quantization.keras.quantize_model(base_model) #faz o modelo reconhecer que será quantizado - simula 'erros' de arredondamento para que o backpropagation ajuste os pesos de modo a compensar a posterior quantização\n",
    "\n",
    "q_aware_model.compile(\n",
    "    optimizer='adam', #ajusta learning rate para cada neurônio individualmente baseado na necessidade\n",
    "    loss='sparse_categorical_crossentropy', #definição da função de custo\n",
    "    metrics=['accuracy'] #porcentagem de imagens que o modelo está acertando\n",
    ")\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping( #parada antecipada\n",
    "    monitor='val_loss', #valor monitorado \n",
    "    patience=5, # se por 5 épocas a perda não cair, ele para\n",
    "    restore_best_weights=True # garante que o modelo final seja o melhor de todos\n",
    ")\n",
    "\n",
    "print(\"Iniciando Treinamento com QAT...\")\n",
    "q_aware_model.fit(train_ds, validation_data=val_ds, epochs=30, callbacks = [early_stop]) #treinamento do modelo com QAT, até 30 épocas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76f0e43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\paulo\\AppData\\Local\\Temp\\tmpyh5jkktz\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\paulo\\AppData\\Local\\Temp\\tmpyh5jkktz\\assets\n",
      "C:\\Users\\paulo\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\lite\\python\\convert.py:863: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo convertido com sucesso para modelo_placa_int8.tflite!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#um gerador de dados representativos, pega amostras para determinar os valores máximos e mínimos das ativações de cada camada, a fim de converter para 8 bits sem perder informação\n",
    "def representative_data_gen():\n",
    "    for input_value, _ in val_ds.take(100): #carrega 100 imagens reais do dataset, for input_value, _ (é uma tupla de iteração input_value = x, _ = y)\n",
    "        yield [input_value] #retorno com yield é um iterador, itera sobre cada input_value/x/imagem\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model) #TFLiteConverter tira dados que são necessários apenas para o treinamento e mantém apenas o necessário para inferência\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT] #Aqui é feito a quantização dos pesos, transforma kernels decimais em int8. converter.optimizations atributo lista\n",
    "converter.representative_dataset = representative_data_gen #Quantização de valores de ativação. converter.representative_dataset atributo que espera função geradora/iteradora. aponta para a referência da função\n",
    "\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8] #Força todas as operações para serem feitas em int8. converter.target_spec.supported_ops subobjeto que guarda regras de operações matemáticas           \n",
    "converter.inference_input_type = tf.int8 #informa que a entrada será de pixels já convertidos para int8\n",
    "converter.inference_output_type = tf.int8 #informa que a saída será de probabilidades já convertidas para int8\n",
    "\n",
    "tflite_model_quant = converter.convert() #Finalizando conversão - String de bytes\n",
    "\n",
    "with open(\"modelo_placa_int8.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model_quant) #escrita do arquivo .tflite no disco \n",
    "\n",
    "print(\"Modelo convertido com sucesso para modelo_placa_int8.tflite!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ebf2b23a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo modelo_placa_int8.h gerado com sucesso!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def bin_to_header(filename, var_name='modelo_placa_int8'): \n",
    "    with open(filename, 'rb') as f: #abre arquivo no modo read bin\n",
    "        data = f.read() #lê e guarda arquivo .tflite na var data\n",
    "    \n",
    "    with open(var_name + '.h', 'w') as f: #cria arquivo e adiciona extensão .h, modo write\n",
    "        f.write(f'unsigned char {var_name}[] = {{\\n  ') # Escreve a declaração do array em C++: tipo 'unsigned char' (1 byte) para garantir compatibilidade\n",
    "        for i, byte in enumerate(data): #retorna iterador e byte\n",
    "            f.write(f'0x{byte:02x}, ') #escreve o byte em hexadecimal seguido de vírgula, :02x força dois dígitos\n",
    "            if (i + 1) % 12 == 0: #quebra linha a cada 12 bytes\n",
    "                f.write('\\n  ')\n",
    "        f.write('\\n};\\n\\n') #fecha array e quebra linha\n",
    "        f.write(f'unsigned int {var_name}_len = {len(data)};\\n') #guarda uma variável com tamanho em bytes do modelo\n",
    " \n",
    "bin_to_header('modelo_placa_int8.tflite')\n",
    "print(\"Arquivo modelo_placa_int8.h gerado com sucesso!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
